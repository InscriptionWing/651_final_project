"""
å…è´¹LLMæ•°æ®ç”Ÿæˆå™¨
ä½¿ç”¨å…è´¹çš„æœ¬åœ°å’Œåœ¨çº¿LLMæœåŠ¡ç”ŸæˆæŠ¤å·¥æ•°æ®
"""

import json
import random
import asyncio
import logging
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import re
import requests
import time
from dataclasses import asdict

from faker import Faker
from tenacity import retry, stop_after_attempt, wait_random_exponential

from carer_data_schema import (
    CarerServiceRecord, ServiceType, ServiceOutcome, LocationType,
    CarerProfile, ParticipantProfile, DataValidator
)
from config import get_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–Faker
fake = Faker(['en_AU', 'zh_CN'])


class FreeLLMDataGenerator:
    """å…è´¹LLMé©±åŠ¨çš„æŠ¤å·¥æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = config or get_config()
        self.gen_config = self.config["data_generation"]
        
        # è®¾ç½®éšæœºç§å­
        random.seed(self.gen_config["random_seed"])
        fake.seed_instance(self.gen_config["random_seed"])
        
        # åˆå§‹åŒ–å…è´¹LLMå®¢æˆ·ç«¯é€‰é¡¹
        self.llm_options = {
            "ollama": self._init_ollama_client,
            "huggingface": self._init_huggingface_client,
            "template": self._init_template_generator,
            "rules": self._init_rules_generator
        }
        
        # é€‰æ‹©å¯ç”¨çš„LLMæ–¹æ³•
        self.active_llm = self._select_available_llm()
        
        # åˆå§‹åŒ–ollamaæ¨¡å‹åç§°ä¸ºé»˜è®¤å€¼ï¼ˆä¼šåœ¨init clientæ—¶æ›´æ–°ï¼‰
        self.ollama_model = "llama2"
        
        # åŠ è½½æ¨¡æ¿
        self.templates = self._load_templates()
        
        # åˆå§‹åŒ–æ•°æ®éªŒè¯å™¨
        self.validator = DataValidator()
        
        # é¢„ç”Ÿæˆçš„æŠ¤å·¥å’Œå‚ä¸è€…æ¡£æ¡ˆ
        self.carers: List[CarerProfile] = []
        self.participants: List[ParticipantProfile] = []
        
        logger.info(f"å…è´¹LLMæ•°æ®ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ–¹æ³•: {self.active_llm}")
    
    def _init_ollama_client(self):
        """åˆå§‹åŒ–Ollamaæœ¬åœ°å®¢æˆ·ç«¯"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if models:
                    # ä¿å­˜æ£€æµ‹åˆ°çš„æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ª
                    self.ollama_model = models[0]['name']
                    logger.info(f"æ£€æµ‹åˆ°Ollamaæ¨¡å‹: {[m['name'] for m in models]}, å°†ä½¿ç”¨: {self.ollama_model}")
                    return "ollama"
        except:
            pass
        return None
    
    def _init_huggingface_client(self):
        """åˆå§‹åŒ–Hugging Faceå…è´¹APIå®¢æˆ·ç«¯"""
        # è¿™é‡Œå¯ä»¥é…ç½®å…è´¹çš„Hugging Face Inference API
        # éœ€è¦ç”¨æˆ·æ³¨å†Œå…è´¹è´¦å·å¹¶è·å–token
        hf_token = self.config.get("huggingface", {}).get("token")
        if hf_token and hf_token != "your_huggingface_token_here":
            return "huggingface"
        return None
    
    def _init_template_generator(self):
        """åˆå§‹åŒ–åŸºäºæ¨¡æ¿çš„ç”Ÿæˆå™¨"""
        return "template"
    
    def _init_rules_generator(self):
        """åˆå§‹åŒ–åŸºäºè§„åˆ™çš„ç”Ÿæˆå™¨"""
        return "rules"
    
    def _select_available_llm(self) -> str:
        """é€‰æ‹©å¯ç”¨çš„LLMæ–¹æ³•"""
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„æ–¹æ³•
        priority_order = ["ollama", "huggingface", "template", "rules"]
        
        for method in priority_order:
            if method in self.llm_options:
                result = self.llm_options[method]()
                if result:
                    return result
        
        # é»˜è®¤ä½¿ç”¨åŸºäºè§„åˆ™çš„ç”Ÿæˆå™¨
        return "rules"
    
    def _load_templates(self) -> List[str]:
        """åŠ è½½å¢å¼ºæ¨¡æ¿"""
        template_file = Path("templates_enhanced.txt")
        if not template_file.exists():
            logger.warning(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_file}")
            return self._get_default_templates()
        
        templates = []
        with open(template_file, 'r', encoding='utf-8') as f:
            content = f.read()
            for line in content.strip().split('\n'):
                if line.strip() and '[' in line and ']' in line:
                    templates.append(line.strip())
        
        logger.info(f"åŠ è½½äº† {len(templates)} ä¸ªæ¨¡æ¿")
        return templates if templates else self._get_default_templates()
    
    def _get_default_templates(self) -> List[str]:
        """è·å–é»˜è®¤æ¨¡æ¿"""
        return [
            "[positive] ä¸ºå‚ä¸è€…æä¾›{service_type}æœåŠ¡ã€‚å‚ä¸è€…é…åˆåº¦è‰¯å¥½ï¼Œç§¯æå‚ä¸å„é¡¹æ´»åŠ¨ã€‚æŠ¤å·¥ä½¿ç”¨äº†{technique}æ–¹æ³•ï¼Œå–å¾—äº†æ»¡æ„çš„æ•ˆæœã€‚æ•´ä¸ªæœåŠ¡è¿‡ç¨‹é¡ºåˆ©è¿›è¡Œã€‚",
            "[neutral] ä¸ºå‚ä¸è€…æä¾›{service_type}æœåŠ¡ã€‚å‚ä¸è€…è¡¨ç°å¹³ç¨³ï¼ŒæŒ‰ç…§è®¡åˆ’å®Œæˆäº†åŸºæœ¬æ´»åŠ¨ã€‚æŠ¤å·¥é‡‡ç”¨äº†{technique}æŠ€æœ¯ï¼Œæ•ˆæœä¸€èˆ¬ã€‚",
            "[negative] ä¸ºå‚ä¸è€…æä¾›{service_type}æœåŠ¡æ—¶é‡åˆ°æŒ‘æˆ˜ã€‚å‚ä¸è€…æƒ…ç»ªæ³¢åŠ¨ï¼Œå¯¹æŸäº›æ´»åŠ¨è¡¨ç°å‡ºæŠ—æ‹’ã€‚æŠ¤å·¥å°è¯•ä½¿ç”¨{technique}æ–¹æ³•ç¼“è§£ï¼Œéœ€è¦åç»­è·Ÿè¿›ã€‚",
            "[positive] æŠ¤å·¥ååŠ©å‚ä¸è€…è¿›è¡Œ{service_type}æ´»åŠ¨ã€‚å‚ä¸è€…è¡¨ç°ç§¯æï¼Œä¸»åŠ¨é…åˆå„é¡¹æ”¯æŒæªæ–½ã€‚é€šè¿‡{technique}ç­–ç•¥çš„å®æ–½ï¼Œè¾¾åˆ°äº†é¢„æœŸçš„æŠ¤ç†ç›®æ ‡ã€‚",
            "[neutral] æŠ¤å·¥ä¸ºå‚ä¸è€…æä¾›{service_type}æ”¯æŒã€‚å‚ä¸è€…çŠ¶æ€ç¨³å®šï¼Œèƒ½å¤Ÿé…åˆå®Œæˆå¿…è¦çš„æŠ¤ç†æ´»åŠ¨ã€‚é‡‡ç”¨{technique}æ–¹æ³•è¿›è¡Œå¹²é¢„ã€‚",
            "[negative] æŠ¤å·¥åœ¨ä¸ºå‚ä¸è€…æä¾›{service_type}æœåŠ¡è¿‡ç¨‹ä¸­é‡åˆ°å›°éš¾ã€‚å‚ä¸è€…éœ€è¦é¢å¤–çš„è€å¿ƒå’Œæ”¯æŒã€‚è™½ç„¶ä½¿ç”¨äº†{technique}æŠ€æœ¯ï¼Œä½†ä»éœ€è¦æŒç»­å…³æ³¨ã€‚"
        ]
    
    def generate_carer_profiles(self, count: int = 50) -> List[CarerProfile]:
        """ç”ŸæˆæŠ¤å·¥æ¡£æ¡ˆ"""
        profiles = []
        config = self.config["carer_profile"]
        
        for i in range(count):
            carer_id = f"CR{random.randint(100000, 999999):06d}"
            
            profile = CarerProfile(
                carer_id=carer_id,
                first_name=fake.first_name(),
                last_name=fake.last_name(),
                certification_level=random.choice(config["certification_levels"]),
                years_experience=random.randint(*config["experience_range"]),
                specializations=random.sample(
                    config["specializations"], 
                    random.randint(1, 3)
                ),
                available_hours_per_week=random.randint(*config["hours_range"]),
                languages=random.sample(
                    config["languages"],
                    random.randint(1, 2)
                )
            )
            profiles.append(profile)
        
        logger.info(f"ç”Ÿæˆäº† {len(profiles)} ä¸ªæŠ¤å·¥æ¡£æ¡ˆ")
        return profiles
    
    def generate_participant_profiles(self, count: int = 100) -> List[ParticipantProfile]:
        """ç”Ÿæˆå‚ä¸è€…æ¡£æ¡ˆ"""
        profiles = []
        config = self.config["participant_profile"]
        
        for i in range(count):
            participant_id = f"PT{random.randint(100000, 999999):06d}"
            
            profile = ParticipantProfile(
                participant_id=participant_id,
                age_group=random.choice(config["age_groups"]),
                disability_type=random.choice(config["disability_types"]),
                support_level=random.choice(config["support_levels"]),
                communication_preferences=random.sample(
                    config["communication_preferences"],
                    random.randint(1, 2)
                ),
                mobility_requirements=random.choice([
                    [], ["wheelchair"], ["walking aid"], ["transfer assistance"]
                ])
            )
            profiles.append(profile)
        
        logger.info(f"ç”Ÿæˆäº† {len(profiles)} ä¸ªå‚ä¸è€…æ¡£æ¡ˆ")
        return profiles
    
    @retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=10))
    async def _call_ollama(self, prompt: str) -> str:
        """è°ƒç”¨Ollamaæœ¬åœ°æ¨¡å‹"""
        try:
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            model_name = getattr(self, 'ollama_model', 'llama2')
            
            data = {
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 500
                }
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                raise Exception(f"Ollama APIé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Ollamaè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    @retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=10))
    async def _call_huggingface(self, prompt: str) -> str:
        """è°ƒç”¨Hugging Faceå…è´¹API"""
        try:
            # ä½¿ç”¨å…è´¹çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
            api_url = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
            
            headers = {
                "Authorization": f"Bearer {self.config.get('huggingface', {}).get('token', '')}",
                "Content-Type": "application/json"
            }
            
            data = {
                "inputs": prompt,
                "parameters": {
                    "max_length": 200,
                    "temperature": 0.7,
                    "do_sample": True
                }
            }
            
            response = requests.post(api_url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    return result[0].get("generated_text", "").strip()
                return ""
            else:
                raise Exception(f"Hugging Face APIé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Hugging Faceè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _generate_with_template(self, 
                               service_type: ServiceType,
                               participant_profile: ParticipantProfile,
                               carer_profile: CarerProfile,
                               outcome: ServiceOutcome) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ•°æ®"""
        
        # é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        outcome_templates = [t for t in self.templates if f"[{outcome.value}]" in t]
        if not outcome_templates:
            outcome_templates = [t for t in self.templates if "[positive]" in t]
        
        template = random.choice(outcome_templates)
        
        # æå–æ¨¡æ¿å†…å®¹
        if ']' in template:
            narrative_template = template[template.find(']')+1:].strip()
        else:
            narrative_template = template
        
        # å¡«å……æ¨¡æ¿å˜é‡
        techniques = [
            "æ¸è¿›å¼å¼•å¯¼", "æ­£å‘å¼ºåŒ–", "ç»“æ„åŒ–æ”¯æŒ", "æ„Ÿå®˜è°ƒèŠ‚",
            "è®¤çŸ¥é‡æ„", "è¡Œä¸ºå¡‘é€ ", "ç¯å¢ƒé€‚åº”", "æ²Ÿé€šè¾…åŠ©"
        ]
        
        locations = [
            "å‚ä¸è€…å®¶ä¸­å®¢å…", "ç¤¾åŒºä¸­å¿ƒæ´»åŠ¨å®¤", "åº·å¤è®­ç»ƒå®¤", 
            "æˆ·å¤–èŠ±å›­åŒºåŸŸ", "å®‰é™çš„é˜…è¯»è§’", "ä¸“ç”¨æ²»ç–—å®¤"
        ]
        
        # ç”Ÿæˆå…·ä½“çš„å™è¿°
        try:
            narrative = narrative_template.format(
                service_type=service_type.value,
                technique=random.choice(techniques),
                participant_name=fake.first_name(),
                location=random.choice(locations)
            )
        except KeyError:
            # å¦‚æœæ¨¡æ¿æ ¼å¼ä¸åŒ¹é…ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            narrative = f"ä¸ºå‚ä¸è€…æä¾›{service_type.value}æœåŠ¡ã€‚æŠ¤å·¥é‡‡ç”¨{random.choice(techniques)}æ–¹æ³•ï¼Œåœ¨{random.choice(locations)}è¿›è¡Œæ”¯æŒæ´»åŠ¨ã€‚"
        
        # ç¡®ä¿å™è¿°é•¿åº¦åˆé€‚
        if len(narrative) < 50:
            narrative += f" æŠ¤å·¥é‡‡ç”¨ä¸“ä¸šçš„{random.choice(techniques)}ç­–ç•¥ï¼Œç¡®ä¿æœåŠ¡è´¨é‡ã€‚"
        elif len(narrative) > 500:
            narrative = narrative[:497] + "..."
        
        # ç”Ÿæˆå…¶ä»–å­—æ®µ
        support_techniques = random.sample(techniques, random.randint(1, 3))
        
        challenges = []
        if outcome in [ServiceOutcome.NEGATIVE, ServiceOutcome.INCOMPLETE]:
            challenge_options = [
                "å‚ä¸è€…æƒ…ç»ªæ³¢åŠ¨", "ç¯å¢ƒå™ªéŸ³å¹²æ‰°", "æ²Ÿé€šå›°éš¾",
                "æ³¨æ„åŠ›åˆ†æ•£", "èº«ä½“ä¸é€‚", "æ—¶é—´å‹åŠ›"
            ]
            challenges = random.sample(challenge_options, random.randint(1, 2))
        
        participant_responses = {
            ServiceOutcome.POSITIVE: ["ç§¯æé…åˆ", "è¡¨ç°å‡ºå…´è¶£", "ä¸»åŠ¨å‚ä¸"],
            ServiceOutcome.NEUTRAL: ["åŸºæœ¬é…åˆ", "è¡¨ç°å¹³ç¨³", "æ— ç‰¹æ®Šååº”"],
            ServiceOutcome.NEGATIVE: ["è¡¨ç°æŠ—æ‹’", "æƒ…ç»ªä¸ç¨³", "éœ€è¦é¢å¤–æ”¯æŒ"],
            ServiceOutcome.INCOMPLETE: ["ä¸­é€”åœæ­¢", "æ³¨æ„åŠ›ä¸é›†ä¸­", "éœ€è¦ä¼‘æ¯"]
        }
        
        return {
            "narrative_notes": narrative,
            "service_outcome": outcome.value,
            "location_details": random.choice(locations),
            "support_techniques_used": support_techniques,
            "challenges_encountered": challenges,
            "participant_response": random.choice(participant_responses.get(outcome, ["æ— ç‰¹æ®Šååº”"])),
            "follow_up_required": outcome in [ServiceOutcome.NEGATIVE, ServiceOutcome.INCOMPLETE]
        }
    
    def _generate_with_rules(self,
                            service_type: ServiceType,
                            participant_profile: ParticipantProfile,
                            carer_profile: CarerProfile,
                            outcome: ServiceOutcome) -> Dict[str, Any]:
        """ä½¿ç”¨è§„åˆ™ç”Ÿæˆæ•°æ®"""
        
        # åŸºäºæœåŠ¡ç±»å‹çš„å™è¿°æ¨¡æ¿
        service_narratives = {
            ServiceType.PERSONAL_CARE: [
                "ååŠ©å‚ä¸è€…è¿›è¡Œä¸ªäººå«ç”ŸæŠ¤ç†ï¼ŒåŒ…æ‹¬æ´—æ¼±ã€ç©¿è¡£ç­‰æ—¥å¸¸æ´»åŠ¨ã€‚",
                "ä¸ºå‚ä¸è€…æä¾›ä¸ªäººæŠ¤ç†æ”¯æŒï¼Œç¡®ä¿å…¶ä¸ªäººå«ç”Ÿå’Œèˆ’é€‚åº¦ã€‚",
                "åœ¨æŠ¤å·¥æŒ‡å¯¼ä¸‹ï¼Œå‚ä¸è€…å®Œæˆäº†ä¸ªäººæŠ¤ç†ä¾‹è¡Œç¨‹åºã€‚"
            ],
            ServiceType.HOUSEHOLD_TASKS: [
                "ååŠ©å‚ä¸è€…å®Œæˆå®¶åŠ¡æ•´ç†ï¼ŒåŒ…æ‹¬æ¸…æ´å’Œç‰©å“å½’ç½®ã€‚",
                "æ”¯æŒå‚ä¸è€…å‚ä¸å®¶åº­ç»´æŠ¤æ´»åŠ¨ï¼ŒåŸ¹å…»ç”Ÿæ´»æŠ€èƒ½ã€‚",
                "æŒ‡å¯¼å‚ä¸è€…è¿›è¡ŒåŸºç¡€å®¶åŠ¡ç®¡ç†ï¼Œæé«˜ç‹¬ç«‹ç”Ÿæ´»èƒ½åŠ›ã€‚"
            ],
            ServiceType.COMMUNITY_ACCESS: [
                "é™ªåŒå‚ä¸è€…å‚ä¸ç¤¾åŒºæ´»åŠ¨ï¼Œä¿ƒè¿›ç¤¾ä¼šèå…¥ã€‚",
                "æ”¯æŒå‚ä¸è€…åœ¨ç¤¾åŒºç¯å¢ƒä¸­çš„æ´»åŠ¨å‚ä¸å’Œäº’åŠ¨ã€‚",
                "ååŠ©å‚ä¸è€…é€‚åº”ç¤¾åŒºç¯å¢ƒï¼Œå»ºç«‹ç¤¾ä¼šè”ç³»ã€‚"
            ],
            ServiceType.TRANSPORT: [
                "ä¸ºå‚ä¸è€…æä¾›äº¤é€šååŠ©ï¼Œç¡®ä¿å®‰å…¨åˆ°è¾¾ç›®çš„åœ°ã€‚",
                "é™ªåŒå‚ä¸è€…è¿›è¡Œå¿…è¦çš„å‡ºè¡Œï¼Œæä¾›é€”ä¸­æ”¯æŒã€‚",
                "ååŠ©å‚ä¸è€…ä½¿ç”¨å…¬å…±äº¤é€šæˆ–å®‰æ’ä¸“è½¦æœåŠ¡ã€‚"
            ]
        }
        
        # é€‰æ‹©åŸºç¡€å™è¿°
        base_narratives = service_narratives.get(service_type, [
            f"ä¸ºå‚ä¸è€…æä¾›{service_type.value}æœåŠ¡æ”¯æŒã€‚"
        ])
        base_narrative = random.choice(base_narratives)
        
        # æ ¹æ®ç»“æœæ·»åŠ å…·ä½“æè¿°
        outcome_descriptions = {
            ServiceOutcome.POSITIVE: [
                "å‚ä¸è€…ç§¯æé…åˆï¼Œé¡ºåˆ©å®Œæˆäº†æ‰€æœ‰è®¡åˆ’æ´»åŠ¨ã€‚",
                "æ•´ä¸ªè¿‡ç¨‹è¿›è¡Œé¡ºåˆ©ï¼Œå‚ä¸è€…è¡¨ç°å‡ºè‰¯å¥½çš„å‚ä¸åº¦ã€‚",
                "å‚ä¸è€…ååº”ç§¯æï¼Œè¾¾åˆ°äº†é¢„æœŸçš„æœåŠ¡ç›®æ ‡ã€‚"
            ],
            ServiceOutcome.NEUTRAL: [
                "å‚ä¸è€…è¡¨ç°å¹³ç¨³ï¼ŒæŒ‰è®¡åˆ’å®Œæˆäº†åŸºæœ¬æ´»åŠ¨ã€‚",
                "æœåŠ¡è¿‡ç¨‹æ­£å¸¸ï¼Œå‚ä¸è€…é…åˆåº¦ä¸€èˆ¬ã€‚",
                "æ´»åŠ¨æŒ‰æ—¢å®šè®¡åˆ’è¿›è¡Œï¼Œæ— ç‰¹æ®Šæƒ…å†µã€‚"
            ],
            ServiceOutcome.NEGATIVE: [
                "å‚ä¸è€…æƒ…ç»ªæ³¢åŠ¨è¾ƒå¤§ï¼Œéœ€è¦é¢å¤–çš„è€å¿ƒå’Œæ”¯æŒã€‚",
                "é‡åˆ°ä¸€äº›æŒ‘æˆ˜ï¼Œå‚ä¸è€…å¯¹æŸäº›æ´»åŠ¨è¡¨ç°å‡ºæŠ—æ‹’ã€‚",
                "æœåŠ¡è¿‡ç¨‹ä¸­å‡ºç°å›°éš¾ï¼Œéœ€è¦è°ƒæ•´ç­–ç•¥ã€‚"
            ],
            ServiceOutcome.INCOMPLETE: [
                "ç”±äºå‚ä¸è€…çŠ¶æ€é—®é¢˜ï¼Œæ´»åŠ¨æœªèƒ½å®Œå…¨å®Œæˆã€‚",
                "æœåŠ¡è¿‡ç¨‹ä¸­æ–­ï¼Œéœ€è¦é‡æ–°å®‰æ’æ—¶é—´ã€‚",
                "å‚ä¸è€…éœ€è¦ä¼‘æ¯ï¼Œæ´»åŠ¨æå‰ç»“æŸã€‚"
            ]
        }
        
        outcome_desc = random.choice(outcome_descriptions.get(outcome, ["æ´»åŠ¨æ­£å¸¸è¿›è¡Œã€‚"]))
        
        # æ·»åŠ æŠ€æœ¯æè¿°
        techniques = [
            "é‡‡ç”¨æ¸è¿›å¼å¼•å¯¼æ–¹æ³•", "ä½¿ç”¨æ­£å‘å¼ºåŒ–ç­–ç•¥", "è¿ç”¨ç»“æ„åŒ–æ”¯æŒæŠ€æœ¯",
            "å®æ–½æ„Ÿå®˜è°ƒèŠ‚æ–¹æ¡ˆ", "åº”ç”¨è®¤çŸ¥é‡æ„æŠ€å·§", "é‡‡ç”¨ç¯å¢ƒé€‚åº”ç­–ç•¥"
        ]
        technique_desc = random.choice(techniques)
        
        # ç»„åˆå®Œæ•´å™è¿°
        full_narrative = f"{base_narrative} {outcome_desc} {technique_desc}ï¼Œç¡®ä¿æœåŠ¡è´¨é‡å’Œå‚ä¸è€…èˆ’é€‚åº¦ã€‚"
        
        # ç”Ÿæˆå…¶ä»–å­—æ®µ
        support_techniques = random.sample([
            "è§†è§‰æç¤º", "å£è¯­æŒ‡å¯¼", "ç‰©ç†ååŠ©", "ç¯å¢ƒè°ƒæ•´",
            "è¡Œä¸ºå¼ºåŒ–", "æ„Ÿå®˜æ”¯æŒ", "æ—¶é—´ç®¡ç†", "ç¤¾äº¤æŠ€èƒ½è®­ç»ƒ"
        ], random.randint(1, 3))
        
        challenges = []
        if outcome in [ServiceOutcome.NEGATIVE, ServiceOutcome.INCOMPLETE]:
            challenges = random.sample([
                "å‚ä¸è€…æƒ…ç»ªç®¡ç†", "ç¯å¢ƒé€‚åº”å›°éš¾", "æ²Ÿé€šéšœç¢",
                "æ³¨æ„åŠ›ç»´æŒ", "ä½“åŠ›é™åˆ¶", "æ—¶é—´å®‰æ’å†²çª"
            ], random.randint(1, 2))
        
        locations = [
            f"{random.choice(['å‚ä¸è€…å®¶ä¸­', 'ç¤¾åŒºä¸­å¿ƒ', 'åº·å¤ä¸­å¿ƒ', 'æˆ·å¤–åœºæ‰€'])}çš„{random.choice(['å®¢å…', 'æ´»åŠ¨å®¤', 'è®­ç»ƒåŒº', 'å®‰é™è§’è½'])}"
        ]
        
        participant_responses = {
            ServiceOutcome.POSITIVE: ["ç§¯æé…åˆ", "ä¸»åŠ¨å‚ä¸", "è¡¨ç°å‡ºè‰²"],
            ServiceOutcome.NEUTRAL: ["åŸºæœ¬é…åˆ", "è¡¨ç°ç¨³å®š", "æ­£å¸¸å‚ä¸"],
            ServiceOutcome.NEGATIVE: ["éœ€è¦é¼“åŠ±", "æƒ…ç»ªæ³¢åŠ¨", "éœ€è¦æ”¯æŒ"],
            ServiceOutcome.INCOMPLETE: ["éœ€è¦ä¼‘æ¯", "æ³¨æ„åŠ›åˆ†æ•£", "çŠ¶æ€ä¸ä½³"]
        }
        
        return {
            "narrative_notes": full_narrative,
            "service_outcome": outcome.value,
            "location_details": random.choice(locations),
            "support_techniques_used": support_techniques,
            "challenges_encountered": challenges,
            "participant_response": random.choice(participant_responses.get(outcome, ["æ­£å¸¸ååº”"])),
            "follow_up_required": outcome in [ServiceOutcome.NEGATIVE, ServiceOutcome.INCOMPLETE]
        }
    
    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨é€‰å®šçš„LLMæ–¹æ³•"""
        if self.active_llm == "ollama":
            return await self._call_ollama(prompt)
        elif self.active_llm == "huggingface":
            return await self._call_huggingface(prompt)
        else:
            # å¯¹äºtemplateå’Œrulesæ–¹æ³•ï¼Œæˆ‘ä»¬ä¸éœ€è¦è°ƒç”¨å¤–éƒ¨LLM
            return ""
    
    async def generate_service_record(self,
                                    carer: CarerProfile,
                                    participant: ParticipantProfile,
                                    service_date: date,
                                    service_type: ServiceType) -> Optional[CarerServiceRecord]:
        """ç”Ÿæˆå•æ¡æœåŠ¡è®°å½•"""
        
        try:
            # ç”ŸæˆåŸºç¡€è®°å½•æ•°æ®
            record_id = f"SR{random.randint(10000000, 99999999):08d}"
            
            # ç¡®å®šæœåŠ¡ç»“æœï¼ˆåŸºäºæƒé‡ï¼‰
            outcome_weights = self.config["service"]["outcome_weights"]
            outcomes = list(ServiceOutcome)
            weights = [outcome_weights.get(oc.value, 0.1) for oc in outcomes]
            service_outcome = random.choices(outcomes, weights=weights)[0]
            
            # ç¡®å®šæœåŠ¡æ—¶é•¿
            duration_ranges = self.config["service"]["duration_ranges"]
            duration_range = duration_ranges.get(service_type.value, (1.0, 4.0))
            duration = round(random.uniform(*duration_range), 2)
            
            # ç¡®å®šåœ°ç‚¹
            location_weights = self.config["location"]["location_weights"]
            location_types = list(LocationType)
            loc_weights = [location_weights.get(lt.value, 0.01) for lt in location_types]
            location_type = random.choices(location_types, weights=loc_weights)[0]
            
            # æ ¹æ®é€‰å®šçš„æ–¹æ³•ç”Ÿæˆå†…å®¹
            if self.active_llm in ["template", "rules"]:
                if self.active_llm == "template":
                    llm_data = self._generate_with_template(
                        service_type, participant, carer, service_outcome
                    )
                else:  # rules
                    llm_data = self._generate_with_rules(
                        service_type, participant, carer, service_outcome
                    )
            else:
                # ä½¿ç”¨å¤–éƒ¨LLM
                prompt = self._build_generation_prompt(service_type, participant, carer)
                llm_response = await self._call_llm(prompt)
                
                try:
                    llm_data = json.loads(llm_response)
                except:
                    # å¦‚æœLLMå“åº”è§£æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™ç”Ÿæˆ
                    llm_data = self._generate_with_rules(
                        service_type, participant, carer, service_outcome
                    )
            
            # åˆ›å»ºæœåŠ¡è®°å½•
            record = CarerServiceRecord(
                record_id=record_id,
                carer_id=carer.carer_id,
                participant_id=participant.participant_id,
                service_date=service_date,
                service_type=service_type,
                duration_hours=duration,
                narrative_notes=llm_data.get("narrative_notes", ""),
                location_type=location_type,
                location_details=llm_data.get("location_details"),
                service_outcome=ServiceOutcome(llm_data.get("service_outcome", service_outcome.value)),
                support_techniques_used=llm_data.get("support_techniques_used", []),
                challenges_encountered=llm_data.get("challenges_encountered", []),
                participant_response=llm_data.get("participant_response"),
                follow_up_required=llm_data.get("follow_up_required", False)
            )
            
            # éªŒè¯è®°å½•
            errors = self.validator.validate_service_record(record)
            if errors:
                logger.warning(f"è®°å½•éªŒè¯å¤±è´¥: {errors}")
                return None
            
            return record
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæœåŠ¡è®°å½•å¤±è´¥: {e}")
            return None
    
    def _build_generation_prompt(self, 
                                service_type: ServiceType,
                                participant_profile: ParticipantProfile,
                                carer_profile: CarerProfile) -> str:
        """æ„å»ºæ•°æ®ç”Ÿæˆæç¤ºï¼ˆç”¨äºå¤–éƒ¨LLMï¼‰"""
        
        prompt = f"""
ç”Ÿæˆä¸€ä¸ªæŠ¤å·¥æœåŠ¡è®°å½•çš„JSONæ•°æ®ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š

å‚ä¸è€…ä¿¡æ¯ï¼š
- å¹´é¾„ç»„: {participant_profile.age_group}
- æ®‹ç–¾ç±»å‹: {participant_profile.disability_type}
- æ”¯æŒçº§åˆ«: {participant_profile.support_level}

æŠ¤å·¥ä¿¡æ¯ï¼š
- è®¤è¯çº§åˆ«: {carer_profile.certification_level}
- ç»éªŒå¹´é™: {carer_profile.years_experience}å¹´
- ä¸“ä¸šé¢†åŸŸ: {', '.join(carer_profile.specializations)}

æœåŠ¡ç±»å‹: {service_type.value}

è¯·ç”ŸæˆJSONæ ¼å¼ï¼š
{{
  "narrative_notes": "è¯¦ç»†çš„æœåŠ¡è®°å½•å™è¿°ï¼ˆ100-300å­—ç¬¦ï¼‰",
  "service_outcome": "positive/neutral/negative/incompleteä¹‹ä¸€",
  "location_details": "å…·ä½“æœåŠ¡åœ°ç‚¹æè¿°",
  "support_techniques_used": ["ä½¿ç”¨çš„æ”¯æŒæŠ€æœ¯åˆ—è¡¨"],
  "challenges_encountered": ["é‡åˆ°çš„æŒ‘æˆ˜åˆ—è¡¨"],
  "participant_response": "å‚ä¸è€…ååº”æè¿°",
  "follow_up_required": true/false
}}
"""
        
        return prompt
    
    async def generate_dataset(self, size: int = 1000) -> List[CarerServiceRecord]:
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†"""
        logger.info(f"å¼€å§‹ç”Ÿæˆ {size} æ¡æœåŠ¡è®°å½•ï¼ˆä½¿ç”¨{self.active_llm}æ–¹æ³•ï¼‰")
        
        # ç”Ÿæˆæ¡£æ¡ˆ
        self.carers = self.generate_carer_profiles(max(10, size // 20))
        self.participants = self.generate_participant_profiles(max(20, size // 10))
        
        records = []
        batch_size = self.gen_config["default_batch_size"]
        
        # æœåŠ¡ç±»å‹æƒé‡
        service_weights = self.config["service"]["service_types_weights"]
        service_types = list(ServiceType)
        weights = [service_weights.get(st.value, 0.1) for st in service_types]
        
        # æ‰¹é‡ç”Ÿæˆ
        for batch_start in range(0, size, batch_size):
            batch_end = min(batch_start + batch_size, size)
            batch_tasks = []
            
            for i in range(batch_start, batch_end):
                # éšæœºé€‰æ‹©æŠ¤å·¥å’Œå‚ä¸è€…
                carer = random.choice(self.carers)
                participant = random.choice(self.participants)
                
                # ç”ŸæˆæœåŠ¡æ—¥æœŸï¼ˆè¿‡å»90å¤©å†…ï¼‰
                days_ago = random.randint(1, 90)
                service_date = date.today() - timedelta(days=days_ago)
                
                # é€‰æ‹©æœåŠ¡ç±»å‹
                service_type = random.choices(service_types, weights=weights)[0]
                
                # åˆ›å»ºä»»åŠ¡
                task = self.generate_service_record(
                    carer, participant, service_date, service_type
                )
                batch_tasks.append(task)
            
            # æ‰§è¡Œæ‰¹é‡ä»»åŠ¡
            if self.active_llm in ["template", "rules"]:
                # åŒæ­¥æ‰§è¡Œï¼Œå› ä¸ºä¸éœ€è¦å¤–éƒ¨APIè°ƒç”¨
                batch_results = []
                for task in batch_tasks:
                    try:
                        result = await task
                        batch_results.append(result)
                    except Exception as e:
                        batch_results.append(e)
            else:
                # å¼‚æ­¥æ‰§è¡Œ
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # æ”¶é›†æˆåŠŸçš„è®°å½•
            for result in batch_results:
                if isinstance(result, CarerServiceRecord):
                    records.append(result)
                elif isinstance(result, Exception):
                    logger.error(f"æ‰¹é‡ç”Ÿæˆé”™è¯¯: {result}")
            
            logger.info(f"å®Œæˆæ‰¹æ¬¡ {batch_start}-{batch_end}, æˆåŠŸç”Ÿæˆ {len([r for r in batch_results if isinstance(r, CarerServiceRecord)])} æ¡è®°å½•")
        
        logger.info(f"æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼Œå…± {len(records)} æ¡æœ‰æ•ˆè®°å½•")
        return records
    
    def save_dataset(self, 
                    records: List[CarerServiceRecord], 
                    filename_prefix: str = "free_llm_carers_data") -> Dict[str, str]:
        """ä¿å­˜æ•°æ®é›†åˆ°å¤šç§æ ¼å¼"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("output")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = {}
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_dicts = [record.to_dict() for record in records]
        
        # ä¿å­˜JSON
        json_file = output_dir / f"{filename_prefix}_{timestamp}_{len(records)}records.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data_dicts, f, ensure_ascii=False, indent=2, default=str)
        saved_files["json"] = str(json_file)
        
        # ä¿å­˜JSONL
        jsonl_file = output_dir / f"{filename_prefix}_{timestamp}_{len(records)}records.jsonl"
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for record_dict in data_dicts:
                f.write(json.dumps(record_dict, ensure_ascii=False, default=str) + '\n')
        saved_files["jsonl"] = str(jsonl_file)
        
        # ä¿å­˜CSV
        try:
            import pandas as pd
            df = pd.DataFrame(data_dicts)
            csv_file = output_dir / f"{filename_prefix}_{timestamp}_{len(records)}records.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            saved_files["csv"] = str(csv_file)
        except ImportError:
            logger.warning("pandasæœªå®‰è£…ï¼Œè·³è¿‡CSVå¯¼å‡º")
        
        logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {saved_files}")
        return saved_files


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå…è´¹LLMæ•°æ®ç”Ÿæˆ"""
    generator = FreeLLMDataGenerator()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_size = 50
    logger.info(f"ç”Ÿæˆæµ‹è¯•æ•°æ®é›†ï¼ˆ{test_size}æ¡è®°å½•ï¼‰")
    
    records = await generator.generate_dataset(test_size)
    
    if records:
        # ä¿å­˜æ•°æ®
        saved_files = generator.save_dataset(records, "free_llm_test_data")
        
        # æ‰“å°è´¨é‡æŠ¥å‘Š
        quality_report = generator.validator.validate_data_quality(records)
        logger.info(f"æ•°æ®è´¨é‡æŠ¥å‘Š: {quality_report}")
        
        # æ˜¾ç¤ºç¤ºä¾‹è®°å½•
        if records:
            logger.info("ç¤ºä¾‹è®°å½•:")
            print(records[0].to_json())
            
        print(f"\nâœ… å…è´¹LLMæ•°æ®ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“Š ç”Ÿæˆè®°å½•æ•°: {len(records)}")
        print(f"ğŸ”§ ä½¿ç”¨æ–¹æ³•: {generator.active_llm}")
        print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶:")
        for format_type, filepath in saved_files.items():
            print(f"   {format_type}: {filepath}")
    else:
        logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")


if __name__ == "__main__":
    asyncio.run(main())
