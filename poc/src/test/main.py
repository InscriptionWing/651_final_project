"""
æŠ¤å·¥æ•°æ®ç”Ÿæˆé¡¹ç›®ä¸»ç¨‹åº
NDISæŠ¤å·¥æœåŠ¡è®°å½•åˆæˆæ•°æ®ç”Ÿæˆå™¨
"""

import asyncio
import logging
import argparse
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, List

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/generator.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import get_config
from llm_data_generator import LLMDataGenerator
from free_llm_generator import FreeLLMDataGenerator
from data_validator import ComprehensiveValidator
from carer_data_schema import CarerServiceRecord


class CarersDataProject:
    """æŠ¤å·¥æ•°æ®ç”Ÿæˆé¡¹ç›®ä¸»ç±»"""
    
    def __init__(self, config_override: Optional[dict] = None, free_mode: bool = False):
        """åˆå§‹åŒ–é¡¹ç›®"""
        self.config = get_config()
        if config_override:
            self.config.update(config_override)
        
        self.free_mode = free_mode
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self._create_directories()
        
        # åˆå§‹åŒ–ç»„ä»¶
        if free_mode:
            self.generator = FreeLLMDataGenerator(self.config)
            logger.info("æŠ¤å·¥æ•°æ®ç”Ÿæˆé¡¹ç›®åˆå§‹åŒ–å®Œæˆï¼ˆå…è´¹æ¨¡å¼ï¼‰")
        else:
            self.generator = LLMDataGenerator(self.config)
            logger.info("æŠ¤å·¥æ•°æ®ç”Ÿæˆé¡¹ç›®åˆå§‹åŒ–å®Œæˆï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰")
            
        self.validator = ComprehensiveValidator(self.config)
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            Path(self.config["output"]["output_dir"]),
            Path("logs"),
            Path("reports")
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            logger.debug(f"åˆ›å»ºç›®å½•: {directory}")
    
    async def generate_dataset(self, 
                             size: int = 1000,
                             validate: bool = True,
                             save_formats: List[str] = None) -> dict:
        """ç”Ÿæˆæ•°æ®é›†å¹¶è¿›è¡ŒéªŒè¯"""
        
        logger.info(f"å¼€å§‹ç”Ÿæˆ {size} æ¡æŠ¤å·¥æœåŠ¡è®°å½•")
        start_time = datetime.now()
        
        try:
            # ç”Ÿæˆæ•°æ®
            records = await self.generator.generate_dataset(size)
            
            if not records:
                raise ValueError("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
            
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(records)} æ¡è®°å½•")
            
            # ä¿å­˜æ•°æ®
            save_formats = save_formats or ["json", "csv", "jsonl"]
            saved_files = self.generator.save_dataset(records, "carers_synthetic_data")
            
            result = {
                "success": True,
                "generated_records": len(records),
                "target_size": size,
                "generation_time": (datetime.now() - start_time).total_seconds(),
                "saved_files": saved_files,
                "validation_results": None
            }
            
            # æ•°æ®éªŒè¯
            if validate and records:
                logger.info("å¼€å§‹æ•°æ®éªŒè¯...")
                validation_start = datetime.now()
                
                validation_results = self.validator.comprehensive_validation(records)
                validation_time = (datetime.now() - validation_start).total_seconds()
                
                # ä¿å­˜éªŒè¯æŠ¥å‘Š
                report_file = self.validator.save_validation_report(
                    validation_results, 
                    f"validation_report_{len(records)}records.json"
                )
                
                result["validation_results"] = validation_results
                result["validation_time"] = validation_time
                result["validation_report"] = report_file
                
                logger.info(f"æ•°æ®éªŒè¯å®Œæˆï¼Œæ€»ä½“è¯„åˆ†: {validation_results.get('overall_score', 0)}")
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            summary_report = self._generate_summary_report(result)
            result["summary_report"] = summary_report
            
            logger.info(f"æ•°æ®ç”Ÿæˆé¡¹ç›®å®Œæˆï¼Œæ€»ç”¨æ—¶: {result['generation_time']:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "generation_time": (datetime.now() - start_time).total_seconds()
            }
    
    def _generate_summary_report(self, result: dict) -> str:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        report_lines = [
            "# NDISæŠ¤å·¥æ•°æ®ç”Ÿæˆé¡¹ç›®æ€»ç»“æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## æ•°æ®ç”Ÿæˆç»“æœ",
            f"- ç›®æ ‡è®°å½•æ•°: {result.get('target_size', 0)}",
            f"- å®é™…ç”Ÿæˆ: {result.get('generated_records', 0)}",
            f"- æˆåŠŸç‡: {(result.get('generated_records', 0) / result.get('target_size', 1) * 100):.1f}%",
            f"- ç”Ÿæˆç”¨æ—¶: {result.get('generation_time', 0):.2f}ç§’",
            ""
        ]
        
        # æ·»åŠ éªŒè¯ç»“æœ
        if result.get("validation_results"):
            validation = result["validation_results"]
            report_lines.extend([
                "## æ•°æ®è´¨é‡éªŒè¯",
                f"- æ€»ä½“è¯„åˆ†: {validation.get('overall_score', 0)}/100",
                f"- éšç§è¯„åˆ†: {validation.get('privacy_analysis', {}).get('anonymization_score', 0)}/100",
                f"- çœŸå®æ€§è¯„åˆ†: {validation.get('utility_analysis', {}).get('realism_score', 0)}/100",
                f"- éªŒè¯ç”¨æ—¶: {result.get('validation_time', 0):.2f}ç§’",
                ""
            ])
            
            # æ·»åŠ å»ºè®®
            recommendations = validation.get("recommendations", [])
            if recommendations:
                report_lines.extend([
                    "## æ”¹è¿›å»ºè®®",
                    *[f"- {rec}" for rec in recommendations],
                    ""
                ])
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        saved_files = result.get("saved_files", {})
        if saved_files:
            report_lines.extend([
                "## ç”Ÿæˆçš„æ–‡ä»¶",
                *[f"- {format_type}: {filepath}" for format_type, filepath in saved_files.items()],
                ""
            ])
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = Path("reports")
        report_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"summary_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return str(report_file)
    
    async def validate_existing_data(self, data_file: str) -> dict:
        """éªŒè¯ç°æœ‰æ•°æ®æ–‡ä»¶"""
        logger.info(f"éªŒè¯ç°æœ‰æ•°æ®æ–‡ä»¶: {data_file}")
        
        try:
            # åŠ è½½æ•°æ®
            records = self._load_data_file(data_file)
            
            # æ‰§è¡ŒéªŒè¯
            validation_results = self.validator.comprehensive_validation(records)
            
            # ä¿å­˜éªŒè¯æŠ¥å‘Š
            report_file = self.validator.save_validation_report(
                validation_results,
                f"validation_existing_{Path(data_file).stem}.json"
            )
            
            return {
                "success": True,
                "validated_records": len(records),
                "validation_results": validation_results,
                "validation_report": report_file
            }
            
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _load_data_file(self, filepath: str) -> List[CarerServiceRecord]:
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        file_path = Path(filepath)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        if file_path.suffix.lower() == '.json':
            return self._load_json_data(file_path)
        elif file_path.suffix.lower() == '.jsonl':
            return self._load_jsonl_data(file_path)
        elif file_path.suffix.lower() == '.csv':
            return self._load_csv_data(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
    
    def _load_json_data(self, file_path: Path) -> List[CarerServiceRecord]:
        """åŠ è½½JSONæ•°æ®"""
        import json
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        records = []
        for item in data:
            try:
                # è¿™é‡Œéœ€è¦ä»å­—å…¸è½¬æ¢å›å¯¹è±¡ï¼Œç®€åŒ–å¤„ç†
                record = CarerServiceRecord(**item)
                records.append(record)
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆè®°å½•: {e}")
        
        return records
    
    def _load_jsonl_data(self, file_path: Path) -> List[CarerServiceRecord]:
        """åŠ è½½JSONLæ•°æ®"""
        import json
        records = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line.strip())
                    record = CarerServiceRecord(**item)
                    records.append(record)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆè®°å½•: {e}")
        
        return records
    
    def _load_csv_data(self, file_path: Path) -> List[CarerServiceRecord]:
        """åŠ è½½CSVæ•°æ®"""
        import pandas as pd
        
        df = pd.read_csv(file_path)
        records = []
        
        for _, row in df.iterrows():
            try:
                # ç®€åŒ–çš„CSVåˆ°å¯¹è±¡è½¬æ¢
                item = row.to_dict()
                record = CarerServiceRecord(**item)
                records.append(record)
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆè®°å½•: {e}")
        
        return records


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="NDISæŠ¤å·¥æ•°æ®ç”Ÿæˆå™¨")
    parser.add_argument("--size", type=int, default=1000, help="ç”Ÿæˆè®°å½•æ•°é‡")
    parser.add_argument("--no-validate", action="store_true", help="è·³è¿‡æ•°æ®éªŒè¯")
    parser.add_argument("--validate-file", type=str, help="éªŒè¯ç°æœ‰æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output-formats", nargs="+", default=["json", "csv", "jsonl"], 
                       help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--config", type=str, help="è‡ªå®šä¹‰é…ç½®æ–‡ä»¶")
    parser.add_argument("--free-mode", action="store_true", help="ä½¿ç”¨å…è´¹LLMæ¨¡å¼")
    parser.add_argument("--check-free-services", action="store_true", help="æ£€æŸ¥å¯ç”¨çš„å…è´¹æœåŠ¡")
    
    args = parser.parse_args()
    
    try:
        # æ£€æŸ¥å…è´¹æœåŠ¡çŠ¶æ€
        if args.check_free_services:
            from free_config import get_setup_instructions, check_available_services
            print(get_setup_instructions())
            available = check_available_services()
            print(f"\nğŸ“Š æœåŠ¡çŠ¶æ€è¯¦æƒ…: {available}")
            return
        
        # åˆå§‹åŒ–é¡¹ç›®
        project = CarersDataProject(free_mode=args.free_mode)
        
        if args.validate_file:
            # éªŒè¯ç°æœ‰æ–‡ä»¶
            result = await project.validate_existing_data(args.validate_file)
        else:
            # ç”Ÿæˆæ–°æ•°æ®
            result = await project.generate_dataset(
                size=args.size,
                validate=not args.no_validate,
                save_formats=args.output_formats
            )
        
        # è¾“å‡ºç»“æœ
        if result["success"]:
            print("\nâœ… é¡¹ç›®æ‰§è¡ŒæˆåŠŸ!")
            if "generated_records" in result:
                print(f"ğŸ“Š ç”Ÿæˆè®°å½•æ•°: {result['generated_records']}")
            if "validation_results" in result and result["validation_results"]:
                print(f"ğŸ¯ è´¨é‡è¯„åˆ†: {result['validation_results']['overall_score']}/100")
            if "saved_files" in result:
                print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
                for format_type, filepath in result["saved_files"].items():
                    print(f"   {format_type}: {filepath}")
        else:
            print(f"\nâŒ é¡¹ç›®æ‰§è¡Œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æ„å¤–é”™è¯¯: {e}")
        logger.exception("ä¸»ç¨‹åºå¼‚å¸¸")
        sys.exit(1)


if __name__ == "__main__":
    # ç¡®ä¿äº‹ä»¶å¾ªç¯åœ¨Windowsä¸Šæ­£å¸¸å·¥ä½œ
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main())
