"""
Ollama APIæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯OllamaæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œä»¥åŠæ£€æµ‹å¯ç”¨æ¨¡å‹
"""

import requests
import json
import time

def test_ollama_connection():
    """æµ‹è¯•Ollamaè¿æ¥"""
    print("ğŸ”„ æµ‹è¯•Ollama APIè¿æ¥...")
    
    try:
        # æµ‹è¯•åŸºæœ¬è¿æ¥
        response = requests.get("http://localhost:11434", timeout=5)
        print(f"âœ… OllamaæœåŠ¡å“åº”: {response.status_code}")
        
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        models_response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if models_response.status_code == 200:
            models_data = models_response.json()
            models = models_data.get("models", [])
            
            if models:
                print(f"ğŸ“¦ æ£€æµ‹åˆ° {len(models)} ä¸ªæ¨¡å‹:")
                for model in models:
                    print(f"   - {model['name']} (å¤§å°: {model.get('size', 'unknown')})")
                
                # æµ‹è¯•ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç”ŸæˆåŠŸèƒ½
                test_model = models[0]['name']
                print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹ '{test_model}' çš„ç”ŸæˆåŠŸèƒ½...")
                
                test_prompt = "è¯·ç”¨ä¸­æ–‡ç®€å•ä»‹ç»ä¸€ä¸‹NDISæŠ¤å·¥æœåŠ¡ã€‚"
                
                generation_data = {
                    "model": test_model,
                    "prompt": test_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "max_tokens": 100
                    }
                }
                
                start_time = time.time()
                gen_response = requests.post(
                    "http://localhost:11434/api/generate",
                    json=generation_data,
                    timeout=60
                )
                end_time = time.time()
                
                if gen_response.status_code == 200:
                    result = gen_response.json()
                    generated_text = result.get("response", "").strip()
                    
                    print(f"âœ… ç”ŸæˆæˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
                    print(f"ğŸ“ ç”Ÿæˆå†…å®¹: {generated_text[:200]}...")
                    
                    return {
                        "status": "success",
                        "models": [m['name'] for m in models],
                        "test_model": test_model,
                        "generation_time": end_time - start_time,
                        "generated_text": generated_text
                    }
                else:
                    print(f"âŒ ç”Ÿæˆå¤±è´¥: HTTP {gen_response.status_code}")
                    print(f"é”™è¯¯ä¿¡æ¯: {gen_response.text}")
                    return {"status": "generation_failed", "error": gen_response.text}
            else:
                print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•æ¨¡å‹")
                print("ğŸ’¡ è¯·å…ˆå®‰è£…æ¨¡å‹ï¼Œä¾‹å¦‚: ollama pull llama2")
                return {"status": "no_models"}
        else:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {models_response.status_code}")
            return {"status": "models_list_failed", "error": models_response.text}
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡")
        print("ğŸ’¡ è¯·ç¡®ä¿Ollamaå·²å®‰è£…å¹¶è¿è¡Œ:")
        print("   1. ä¸‹è½½Ollama: https://ollama.ai/download")
        print("   2. å¯åŠ¨æœåŠ¡: ollama serve")
        print("   3. å®‰è£…æ¨¡å‹: ollama pull llama2")
        return {"status": "connection_failed"}
    
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶")
        return {"status": "timeout"}
    
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return {"status": "error", "error": str(e)}

def recommend_setup_steps():
    """æ¨èè®¾ç½®æ­¥éª¤"""
    print("\nğŸ“‹ Ollamaè®¾ç½®å»ºè®®:")
    print("1. ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")
    print("2. æ¨èå®‰è£…ä»¥ä¸‹æ¨¡å‹ä¹‹ä¸€:")
    print("   - ollama pull llama2         # é€šç”¨æ¨¡å‹ï¼Œ3.8GB")
    print("   - ollama pull mistral        # é«˜è´¨é‡æ¨¡å‹ï¼Œ4.1GB")
    print("   - ollama pull qwen:7b        # ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼Œ4.0GB")
    print("3. éªŒè¯å®‰è£…: ollama list")
    print("4. æµ‹è¯•ç”Ÿæˆ: ollama run llama2")

if __name__ == "__main__":
    print("ğŸ¦™ Ollama API æµ‹è¯•ç¨‹åº")
    print("=" * 50)
    
    result = test_ollama_connection()
    
    if result["status"] == "success":
        print(f"\nâœ… Ollama APIæµ‹è¯•é€šè¿‡!")
        print(f"ğŸ¯ æ¨èä½¿ç”¨æ¨¡å‹: {result['test_model']}")
        print(f"âš¡ ç”Ÿæˆé€Ÿåº¦: {result['generation_time']:.2f}ç§’")
    else:
        print(f"\nâŒ Ollama APIæµ‹è¯•å¤±è´¥: {result['status']}")
        recommend_setup_steps()
    
    print("\n" + "=" * 50)

