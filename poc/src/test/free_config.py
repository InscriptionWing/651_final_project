"""
å…è´¹LLMé…ç½®æ–‡ä»¶
åŒ…å«å„ç§å…è´¹LLMæœåŠ¡çš„é…ç½®é€‰é¡¹
"""

import os
from pathlib import Path
from typing import Dict, List, Any

# é¡¹ç›®åŸºç¡€é…ç½®
PROJECT_CONFIG = {
    "name": "NDIS_Carers_Data_Generator_Free",
    "version": "1.1.0",
    "description": "NDISæŠ¤å·¥æœåŠ¡è®°å½•åˆæˆæ•°æ®ç”Ÿæˆå™¨ - å…è´¹ç‰ˆæœ¬",
    "debug": True
}

# å…è´¹LLMé…ç½®é€‰é¡¹
FREE_LLM_CONFIG = {
    # é¦–é€‰æ–¹æ³•ï¼šOllamaæœ¬åœ°æ¨¡å‹ï¼ˆå®Œå…¨å…è´¹ï¼‰
    "ollama": {
        "enabled": True,
        "base_url": "http://localhost:11434", #api: dc93835ea32d4a8bb097d80471b3f92c.i1gbA9QSQkpwu93m7d0dTop2
        "models": {
            "primary": "llama2",        # æ¨èï¼šè½»é‡çº§ï¼Œé€Ÿåº¦å¿«
            "alternative": "mistral",   # å¤‡é€‰ï¼šè´¨é‡æ›´é«˜
            "chinese": "qwen:7b"        # ä¸­æ–‡æ”¯æŒæ›´å¥½
        },
        "generation_params": {
            "temperature": 0.7,
            "top_p": 0.9,
            "max_tokens": 500,
            "timeout": 30
        }
    },
    
    # å¤‡é€‰ï¼šHugging Faceå…è´¹APIï¼ˆéœ€è¦æ³¨å†Œå…è´¹è´¦å·ï¼‰
    "huggingface": {
        "enabled": True,
        "token": "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",  # éœ€è¦ç”¨æˆ·æ›¿æ¢
        "models": {
            "text_generation": "microsoft/DialoGPT-medium",
            "chinese_model": "THUDM/chatglm-6b",
            "medical_model": "microsoft/BioGPT"
        },
        "api_url_template": "https://api-inference.huggingface.co/models/{model}",
        "generation_params": {
            "max_length": 200,
            "temperature": 0.7,
            "do_sample": True,
            "timeout": 30
        }
    },
    
    # æœ¬åœ°è¿è¡Œçš„å¼€æºæ¨¡å‹ï¼ˆéœ€è¦GPUï¼‰
    "local_models": {
        "enabled": False,  # é»˜è®¤å…³é—­ï¼Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨å¯ç”¨
        "models": {
            "chinese_medical": {
                "name": "ChatGLM-6B",
                "path": "./models/chatglm-6b",
                "requirements": ["torch", "transformers", "sentencepiece"]
            },
            "general": {
                "name": "LLaMA-7B",
                "path": "./models/llama-7b",
                "requirements": ["torch", "transformers", "accelerate"]
            }
        }
    },
    
    # åŸºäºæ¨¡æ¿çš„ç”Ÿæˆï¼ˆæ— éœ€LLMï¼Œè´¨é‡å¾ˆå¥½ï¼‰
    "template_based": {
        "enabled": True,
        "priority": 3,  # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        "template_file": "templates_enhanced.txt",
        "custom_templates": {
            "positive_templates": [
                "ä¸ºå‚ä¸è€…{participant_name}æä¾›{service_type}æœåŠ¡ã€‚å‚ä¸è€…ç§¯æé…åˆï¼Œä½¿ç”¨{technique}æ–¹æ³•å–å¾—è‰¯å¥½æ•ˆæœã€‚",
                "ååŠ©{participant_name}å®Œæˆ{service_type}æ´»åŠ¨ã€‚è¿‡ç¨‹é¡ºåˆ©ï¼Œå‚ä¸è€…è¡¨ç°å‡ºè‰²ï¼Œ{technique}ç­–ç•¥å¾ˆæœ‰æ•ˆã€‚",
                "ä»Šæ—¥ä¸º{participant_name}è¿›è¡Œ{service_type}ã€‚å‚ä¸è€…é…åˆåº¦é«˜ï¼Œé€šè¿‡{technique}è¾¾åˆ°é¢„æœŸç›®æ ‡ã€‚"
            ],
            "neutral_templates": [
                "ä¸º{participant_name}æä¾›{service_type}æœåŠ¡ã€‚å‚ä¸è€…è¡¨ç°å¹³ç¨³ï¼Œé‡‡ç”¨{technique}æ–¹æ³•æŒ‰è®¡åˆ’å®Œæˆã€‚",
                "ååŠ©{participant_name}è¿›è¡Œ{service_type}ã€‚è¿‡ç¨‹æ­£å¸¸ï¼Œä½¿ç”¨{technique}æŠ€æœ¯ï¼Œæ•ˆæœä¸€èˆ¬ã€‚",
                "ä»Šæ—¥{participant_name}çš„{service_type}æœåŠ¡æŒ‰è®¡åˆ’è¿›è¡Œï¼Œè¿ç”¨{technique}æ–¹æ³•ã€‚"
            ],
            "negative_templates": [
                "ä¸º{participant_name}æä¾›{service_type}æœåŠ¡æ—¶é‡åˆ°æŒ‘æˆ˜ã€‚å°è¯•{technique}æ–¹æ³•ï¼Œéœ€è¦è°ƒæ•´ç­–ç•¥ã€‚",
                "ååŠ©{participant_name}è¿›è¡Œ{service_type}é‡åˆ°å›°éš¾ã€‚ä½¿ç”¨{technique}æŠ€æœ¯ç¼“è§£ï¼Œæ•ˆæœæœ‰é™ã€‚",
                "ä»Šæ—¥{participant_name}çš„{service_type}æœåŠ¡ä¸å¤Ÿé¡ºåˆ©ï¼Œé‡‡ç”¨{technique}æ–¹æ³•éœ€è¦æ”¹è¿›ã€‚"
            ]
        }
    },
    
    # åŸºäºè§„åˆ™çš„ç”Ÿæˆï¼ˆå®Œå…¨æœ¬åœ°ï¼Œé€Ÿåº¦æœ€å¿«ï¼‰
    "rule_based": {
        "enabled": True,
        "priority": 4,
        "narrative_components": {
            "openings": [
                "ä¸ºå‚ä¸è€…æä¾›ä¸“ä¸šçš„{service_type}æœåŠ¡ï¼Œ",
                "ä»Šæ—¥ååŠ©å‚ä¸è€…è¿›è¡Œ{service_type}æ´»åŠ¨ï¼Œ",
                "åœ¨æŠ¤å·¥æŒ‡å¯¼ä¸‹ï¼Œå‚ä¸è€…å‚ä¸{service_type}ï¼Œ",
                "æ ¹æ®å‚ä¸è€…éœ€æ±‚ï¼Œå®æ–½{service_type}æ”¯æŒï¼Œ"
            ],
            "process_descriptions": [
                "è¿‡ç¨‹ä¸­é‡‡ç”¨{technique}æ–¹æ³•ï¼Œç¡®ä¿æœåŠ¡è´¨é‡ã€‚",
                "è¿ç”¨{technique}ç­–ç•¥ï¼Œä¿ƒè¿›å‚ä¸è€…ç§¯æå‚ä¸ã€‚",
                "é€šè¿‡{technique}æŠ€æœ¯ï¼Œæä¾›ä¸ªæ€§åŒ–æ”¯æŒã€‚",
                "å®æ–½{technique}æ–¹æ¡ˆï¼Œæ»¡è¶³å‚ä¸è€…éœ€æ±‚ã€‚"
            ],
            "outcomes": {
                "positive": [
                    "å‚ä¸è€…ååº”ç§¯æï¼Œè¾¾åˆ°é¢„æœŸç›®æ ‡ã€‚",
                    "æ´»åŠ¨è¿›è¡Œé¡ºåˆ©ï¼Œå‚ä¸è€…è¡¨ç°å‡ºè‰²ã€‚",
                    "æœåŠ¡æ•ˆæœè‰¯å¥½ï¼Œå‚ä¸è€…æ»¡æ„åº¦é«˜ã€‚"
                ],
                "neutral": [
                    "å‚ä¸è€…è¡¨ç°å¹³ç¨³ï¼Œå®ŒæˆåŸºæœ¬ç›®æ ‡ã€‚",
                    "æ´»åŠ¨æŒ‰è®¡åˆ’è¿›è¡Œï¼Œæ— ç‰¹æ®Šæƒ…å†µã€‚",
                    "æœåŠ¡è¿‡ç¨‹æ­£å¸¸ï¼Œå‚ä¸è€…é…åˆä¸€èˆ¬ã€‚"
                ],
                "negative": [
                    "é‡åˆ°ä¸€äº›æŒ‘æˆ˜ï¼Œéœ€è¦åç»­è·Ÿè¿›ã€‚",
                    "å‚ä¸è€…æƒ…ç»ªæ³¢åŠ¨ï¼Œéœ€è¦é¢å¤–æ”¯æŒã€‚",
                    "æ´»åŠ¨æœªå®Œå…¨è¾¾åˆ°é¢„æœŸï¼Œéœ€è¦è°ƒæ•´ã€‚"
                ]
            }
        }
    }
}

# å…è´¹APIæœåŠ¡é…ç½®
FREE_API_SERVICES = {
    # Google AI Studio (Gemini) - å…è´¹é…é¢
    "google_ai": {
        "enabled": False,  # éœ€è¦ç”¨æˆ·é…ç½®
        "api_key": "your_google_ai_key_here",
        "model": "gemini-pro",
        "free_limit": "60 requests per minute"
    },
    
    # Cohereå…è´¹å±‚
    "cohere": {
        "enabled": False,
        "api_key": "your_cohere_key_here", 
        "model": "command",
        "free_limit": "100 requests per month"
    },
    
    # Together AIå…è´¹é…é¢
    "together_ai": {
        "enabled": False,
        "api_key": "your_together_key_here",
        "models": ["togethercomputer/llama-2-7b-chat"],
        "free_limit": "$25 monthly credit"
    }
}

# æ•°æ®ç”Ÿæˆé…ç½®ï¼ˆé’ˆå¯¹å…è´¹ä½¿ç”¨ä¼˜åŒ–ï¼‰
DATA_GENERATION_CONFIG = {
    "default_batch_size": 20,  # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œé¿å…APIé™åˆ¶
    "min_narrative_length": 50,
    "max_narrative_length": 300,  # å‡å°‘é•¿åº¦ï¼Œæé«˜ç”Ÿæˆé€Ÿåº¦
    "target_dataset_size": 500,   # é»˜è®¤ç›®æ ‡å‡å°‘åˆ°500æ¡
    "max_concurrent_requests": 2,  # å‡å°‘å¹¶å‘ï¼Œé¿å…è§¦å‘é™åˆ¶
    "request_delay": 1.0,         # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    "retry_count": 2,             # å‡å°‘é‡è¯•æ¬¡æ•°
    "random_seed": 42,
    "fallback_enabled": True      # å¯ç”¨å›é€€æœºåˆ¶
}

# æ¨¡å‹è‡ªåŠ¨æ£€æµ‹å’Œé€‰æ‹©ç­–ç•¥
AUTO_SELECTION_STRATEGY = {
    "priority_order": [
        "ollama",           # é¦–é€‰ï¼šæœ¬åœ°Ollama
        "template_based",   # æ¬¡é€‰ï¼šåŸºäºæ¨¡æ¿
        "rule_based",       # å†æ¬¡ï¼šåŸºäºè§„åˆ™
        "huggingface",      # æœ€åï¼šå…è´¹API
    ],
    "fallback_enabled": True,
    "quality_threshold": 0.7,
    "speed_preference": "balanced"  # "fast", "balanced", "quality"
}

# æœ¬åœ°æ¨¡å‹å®‰è£…æŒ‡å—
OLLAMA_SETUP_GUIDE = {
    "installation_steps": [
        "1. ä¸‹è½½å¹¶å®‰è£…Ollama: https://ollama.ai/download",
        "2. å¯åŠ¨OllamaæœåŠ¡",
        "3. å®‰è£…æ¨èæ¨¡å‹: ollama pull llama2",
        "4. å¯é€‰å®‰è£…ä¸­æ–‡æ¨¡å‹: ollama pull qwen:7b",
        "5. æ£€æŸ¥å®‰è£…: ollama list"
    ],
    "recommended_models": {
        "llama2": {
            "size": "3.8GB",
            "description": "Metaçš„Llama 2æ¨¡å‹ï¼Œè‹±æ–‡æ•ˆæœå¥½",
            "command": "ollama pull llama2"
        },
        "mistral": {
            "size": "4.1GB", 
            "description": "Mistral AIæ¨¡å‹ï¼Œä»£ç å’Œæ¨ç†èƒ½åŠ›å¼º",
            "command": "ollama pull mistral"
        },
        "qwen:7b": {
            "size": "4.0GB",
            "description": "é˜¿é‡Œé€šä¹‰åƒé—®ï¼Œä¸­æ–‡æ”¯æŒä¼˜ç§€",
            "command": "ollama pull qwen:7b"
        },
        "codellama": {
            "size": "3.8GB",
            "description": "ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆçš„Llamaç‰ˆæœ¬",
            "command": "ollama pull codellama"
        }
    }
}

# Hugging Faceè®¾ç½®æŒ‡å—
HUGGINGFACE_SETUP_GUIDE = {
    "steps": [
        "1. è®¿é—® https://huggingface.co/ æ³¨å†Œå…è´¹è´¦å·",
        "2. è¿›å…¥ Settings -> Access Tokens",
        "3. åˆ›å»ºæ–°çš„ Read æƒé™ token",
        "4. å°†tokenå¤åˆ¶åˆ°é…ç½®æ–‡ä»¶ä¸­",
        "5. å…è´¹è´¦æˆ·æ¯æœˆæœ‰ä¸€å®šçš„APIè°ƒç”¨é™åˆ¶"
    ],
    "free_models": [
        "microsoft/DialoGPT-medium",
        "gpt2",
        "distilbert-base-uncased",
        "facebook/blenderbot-400M-distill"
    ]
}

def get_free_config() -> Dict[str, Any]:
    """è·å–å…è´¹LLMé…ç½®"""
    return {
        "project": PROJECT_CONFIG,
        "free_llm": FREE_LLM_CONFIG,
        "free_apis": FREE_API_SERVICES,
        "data_generation": DATA_GENERATION_CONFIG,
        "auto_selection": AUTO_SELECTION_STRATEGY,
        "setup_guides": {
            "ollama": OLLAMA_SETUP_GUIDE,
            "huggingface": HUGGINGFACE_SETUP_GUIDE
        }
    }

def check_available_services() -> Dict[str, bool]:
    """æ£€æŸ¥å¯ç”¨çš„å…è´¹LLMæœåŠ¡"""
    available = {}
    
    # æ£€æŸ¥Ollama
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        available["ollama"] = response.status_code == 200
    except:
        available["ollama"] = False
    
    # æ£€æŸ¥Hugging Face token
    hf_token = FREE_LLM_CONFIG["huggingface"]["token"]
    available["huggingface"] = hf_token and hf_token != "your_huggingface_token_here"
    
    # æœ¬åœ°æ–¹æ³•æ€»æ˜¯å¯ç”¨
    available["template_based"] = True
    available["rule_based"] = True
    
    return available

def get_setup_instructions() -> str:
    """è·å–è®¾ç½®è¯´æ˜"""
    available = check_available_services()
    
    instructions = ["ğŸš€ å…è´¹LLMæ•°æ®ç”Ÿæˆå™¨è®¾ç½®æŒ‡å—\n"]
    
    if not available["ollama"]:
        instructions.append("ğŸ“¥ æ¨èè®¾ç½®Ollamaæœ¬åœ°æ¨¡å‹ï¼ˆå®Œå…¨å…è´¹ï¼Œæ— é™åˆ¶ï¼‰:")
        for step in OLLAMA_SETUP_GUIDE["installation_steps"]:
            instructions.append(f"   {step}")
        instructions.append("")
    
    if not available["huggingface"]:
        instructions.append("ğŸ¤— å¯é€‰è®¾ç½®Hugging Faceå…è´¹API:")
        for step in HUGGINGFACE_SETUP_GUIDE["steps"]:
            instructions.append(f"   {step}")
        instructions.append("")
    
    instructions.append("âœ… å½“å‰å¯ç”¨çš„ç”Ÿæˆæ–¹æ³•:")
    for service, is_available in available.items():
        status = "ğŸŸ¢ å¯ç”¨" if is_available else "ğŸ”´ ä¸å¯ç”¨"
        instructions.append(f"   {service}: {status}")
    
    if available["template_based"] or available["rule_based"]:
        instructions.append("\nğŸ’¡ å³ä½¿æ²¡æœ‰é…ç½®å¤–éƒ¨APIï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨åŸºäºæ¨¡æ¿å’Œè§„åˆ™çš„ç”Ÿæˆæ–¹æ³•ï¼")
    
    return "\n".join(instructions)

if __name__ == "__main__":
    # æ˜¾ç¤ºè®¾ç½®æŒ‡å—
    print(get_setup_instructions())
    
    # æ˜¾ç¤ºå¯ç”¨æœåŠ¡
    available = check_available_services()
    print(f"\nå½“å‰å¯ç”¨æœåŠ¡: {available}")

